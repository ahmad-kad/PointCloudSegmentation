{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScanNet Instance Segmentation Data Exploration\n",
    "\n",
    "This notebook explores the ScanNet dataset and analyzes various aspects of the point cloud data for instance segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import open3d as o3d\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.data import ScanNetDataset\n",
    "from src.utils.data_utils import DataProcessor\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('../configs/data_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = ScanNetDataset(\n",
    "    root_dir='../data/scannet',\n",
    "    split='train',\n",
    "    config_path='../configs/data_config.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_statistics(dataset):\n",
    "    \"\"\"Analyze basic statistics of the dataset.\"\"\"\n",
    "    stats = {\n",
    "        'num_scenes': len(dataset),\n",
    "        'points_per_scene': [],\n",
    "        'instances_per_scene': [],\n",
    "        'semantic_classes': set(),\n",
    "        'points_per_instance': []\n",
    "    }\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc='Analyzing dataset'):\n",
    "        data = dataset[i]\n",
    "        \n",
    "        # Count points\n",
    "        stats['points_per_scene'].append(len(data['points']))\n",
    "        \n",
    "        # Count instances\n",
    "        unique_instances = torch.unique(data['instance_labels'])\n",
    "        stats['instances_per_scene'].append(len(unique_instances))\n",
    "        \n",
    "        # Count semantic classes\n",
    "        stats['semantic_classes'].update(torch.unique(data['semantic_labels']).numpy())\n",
    "        \n",
    "        # Count points per instance\n",
    "        for inst_id in unique_instances:\n",
    "            if inst_id == 0:  # Skip background\n",
    "                continue\n",
    "            stats['points_per_instance'].append(\n",
    "                torch.sum(data['instance_labels'] == inst_id).item()\n",
    "            )\n",
    "    \n",
    "    return stats\n",
    "\n",
    "stats = analyze_dataset_statistics(dataset)\n",
    "\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"Number of scenes: {stats['num_scenes']}\")\n",
    "print(f\"Number of semantic classes: {len(stats['semantic_classes'])}\")\n",
    "print(f\"\\nPoints per scene:\")\n",
    "print(f\"  Mean: {np.mean(stats['points_per_scene']):.2f}\")\n",
    "print(f\"  Std: {np.std(stats['points_per_scene']):.2f}\")\n",
    "print(f\"  Min: {np.min(stats['points_per_scene'])}\")\n",
    "print(f\"  Max: {np.max(stats['points_per_scene'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Point Cloud Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of points per scene\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(stats['points_per_scene'], bins=50)\n",
    "plt.title('Distribution of Points per Scene')\n",
    "plt.xlabel('Number of Points')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of instances per scene\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(stats['instances_per_scene'], bins=30)\n",
    "plt.title('Distribution of Instances per Scene')\n",
    "plt.xlabel('Number of Instances')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(data):\n",
    "    \"\"\"Analyze feature distributions for a single scene.\"\"\"\n",
    "    feature_names = ['geometric_features', 'contextual_features']\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5 * len(feature_names)))\n",
    "    \n",
    "    for i, feat_name in enumerate(feature_names):\n",
    "        if feat_name not in data or data[feat_name] is None:\n",
    "            continue\n",
    "            \n",
    "        features = data[feat_name].numpy()\n",
    "        n_features = features.shape[1]\n",
    "        \n",
    "        for j in range(n_features):\n",
    "            plt.subplot(len(feature_names), n_features, i*n_features + j + 1)\n",
    "            plt.hist(features[:, j], bins=50)\n",
    "            plt.title(f'{feat_name}_{j}')\n",
    "            plt.xlabel('Value')\n",
    "            plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze features for a sample scene\n",
    "sample_data = dataset[0]\n",
    "analyze_features(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize 3D Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scene(points, instance_labels, semantic_labels=None):\n",
    "    \"\"\"Visualize point cloud with instance and semantic labels.\"\"\"\n",
    "    # Create color map for instances\n",
    "    unique_instances = torch.unique(instance_labels)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_instances)))[:, :3]\n",
    "    \n",
    "    # Create point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points.numpy())\n",
    "    \n",
    "    # Color by instance\n",
    "    point_colors = np.zeros((len(points), 3))\n",
    "    for i, inst_id in enumerate(unique_instances):\n",
    "        mask = (instance_labels == inst_id)\n",
    "        point_colors[mask] = colors[i]\n",
    "    \n",
    "    pcd.colors = o3d.utility.Vector3dVector(point_colors)\n",
    "    \n",
    "    # Visualize\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "# Visualize sample scene\n",
    "sample_data = dataset[0]\n",
    "visualize_scene(\n",
    "    sample_data['points'],\n",
    "    sample_data['instance_labels'],\n",
    "    sample_data['semantic_labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Instance Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_instance_sizes():\n",
    "    instance_sizes = []\n",
    "    semantic_distribution = {}\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc='Analyzing instances'):\n",
    "        data = dataset[i]\n",
    "        instance_labels = data['instance_labels']\n",
    "        semantic_labels = data['semantic_labels']\n",
    "        \n",
    "        for inst_id in torch.unique(instance_labels):\n",
    "            if inst_id == 0:  # Skip background\n",
    "                continue\n",
    "                \n",
    "            mask = (instance_labels == inst_id)\n",
    "            size = torch.sum(mask).item()\n",
    "            sem_label = semantic_labels[mask][0].item()\n",
    "            \n",
    "            instance_sizes.append(size)\n",
    "            semantic_distribution[sem_label] = semantic_distribution.get(sem_label, 0) + 1\n",
    "    \n",
    "    return instance_sizes, semantic_distribution\n",
    "\n",
    "instance_sizes, semantic_dist = analyze_instance_sizes()\n",
    "\n",
    "# Plot instance size distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(np.log10(instance_sizes), bins=50)\n",
    "plt.title('Distribution of Instance Sizes (log scale)')\n",
    "plt.xlabel('Log10(Number of Points)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Plot semantic class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(semantic_dist.keys(), semantic_dist.values())\n",
    "plt.title('Distribution of Semantic Classes')\n",
    "plt.xlabel('Semantic Class ID')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Spatial Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_spatial_distribution(points):\n",
    "    \"\"\"Analyze spatial distribution of points.\"\"\"\n",
    "    # Create 3D scatter plot\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=points[:, 0],\n",
    "        y=points[:, 1],\n",
    "        z=points[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color=points[:, 2],\n",
    "            colorscale='Viridis',\n",
    "            opacity=0.8\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Spatial Distribution of Points',\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Analyze spatial distribution for a sample scene\n",
    "sample_data = dataset[0]\n",
    "analyze_spatial_distribution(sample_data['points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_correlations(data):\n",
    "    \"\"\"Analyze correlations between different features.\"\"\"\n",
    "    # Combine all features\n",
    "    feature_dict = {}\n",
    "    \n",
    "    if 'geometric_features' in data and data['geometric_features'] is not None:\n",
    "        for i in range(data['geometric_features'].shape[1]):\n",
    "            feature_dict[f'geometric_{i}'] = data['geometric_features'][:, i]\n",
    "            \n",
    "    if 'contextual_features' in data and data['contextual_features'] is not None:\n",
    "        for i in range(data['contextual_features'].shape[1]):\n",
    "            feature_dict[f'contextual_{i}'] = data['contextual_features'][:, i]\n",
    "            \n",
    "    # Create correlation matrix\n",
    "    feature_matrix = np.array(list(feature_dict.values())).T\n",
    "    correlation_matrix = np.corrcoef(feature_matrix.T)\n",
    "    \n",
    "    # Plot correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        xticklabels=list(feature_dict.keys()),\n",
    "        yticklabels=list(feature_dict.keys()),\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        annot=True,\n",
    "        fmt='.2f'\n",
    "    )\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "# Analyze feature correlations for a sample scene\n",
    "sample_data = dataset[0]\n",
    "correlation_matrix = analyze_feature_correlations(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Instance Boundary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_instance_boundaries(data):\n",
    "    \"\"\"Analyze characteristics of instance boundaries.\"\"\"\n",
    "    points = data['points']\n",
    "    instance_labels = data['instance_labels']\n",
    "    \n",
    "    # Create KD-tree for nearest neighbor search\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points.numpy())\n",
    "    kdtree = o3d.geometry.KDTreeFlann(pcd)\n",
    "    \n",
    "    # Find boundary points\n",
    "    boundary_points = []\n",
    "    for i in tqdm(range(len(points)), desc='Analyzing boundaries'):\n",
    "        [_, idx, _] = kdtree.search_knn_vector_3d(pcd.points[i], 30)\n",
    "        neighbor_labels = instance_labels[idx]\n",
    "        if len(torch.unique(neighbor_labels)) > 1:\n",
    "            boundary_points.append(i)\n",
    "    \n",
    "    boundary_points = np.array(boundary_points)\n",
    "    \n",
    "    # Visualize boundary points\n",
    "    colors = np.zeros((len(points), 3))\n",
    "    colors[boundary_points] = [1, 0, 0]  # Red for boundary points\n",
    "    \n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "    \n",
    "    return boundary_points\n",
    "\n",
    "# Analyze boundaries for a sample scene\n",
    "sample_data = dataset[0]\n",
    "boundary_points = analyze_instance_boundaries(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Instance Size vs. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_size_feature_relationship(data):\n",
    "    \"\"\"Analyze relationship between instance size and features.\"\"\"\n",
    "    instance_labels = data['instance_labels']\n",
    "    unique_instances = torch.unique(instance_labels)\n",
    "    \n",
    "    # Calculate instance sizes and mean features\n",
    "    sizes = []\n",
    "    mean_features = []\n",
    "    \n",
    "    for inst_id in unique_instances:\n",
    "        if inst_id == 0:  # Skip background\n",
    "            continue\n",
    "            \n",
    "        mask = (instance_labels == inst_id)\n",
    "        sizes.append(torch.sum(mask).item())\n",
    "        \n",
    "        if 'geometric_features' in data and data['geometric_features'] is not None:\n",
    "            feat_mean = data['geometric_features'][mask].mean(0).numpy()\n",
    "            mean_features.append(feat_mean)\n",
    "    \n",
    "    sizes = np.array(sizes)\n",
    "    mean_features = np.array(mean_features)\n",
    "    \n",
    "    # Plot relationships\n",
    "    if len(mean_features) > 0:\n",
    "        fig, axes = plt.subplots(1, mean_features.shape[1], figsize=(15, 5))\n",
    "        for i in range(mean_features.shape[1]):\n",
    "            axes[i].scatter(np.log10(sizes), mean_features[:, i], alpha=0.6)\n",
    "            axes[i].set_xlabel('Log10(Instance Size)')\n",
    "            axes[i].set_ylabel(f'Feature {i}')\n",
    "            axes[i].set_title(f'Size vs Feature {i}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return sizes, mean_features\n",
    "\n",
    "# Analyze size-feature relationships\n",
    "sample_data = dataset[0]\n",
    "sizes, mean_features = analyze_size_feature_relationship(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Augmentation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(data):\n",
    "    \"\"\"Visualize effects of different data augmentations.\"\"\"\n",
    "    from src.data.augmentation import AugmentationPipeline\n",
    "    \n",
    "    # Initialize augmentation pipeline\n",
    "    augmenter = AugmentationPipeline('../configs/data_config.yaml')\n",
    "    \n",
    "    # Apply different augmentations\n",
    "    augmentations = ['random_rotation', 'random_scale', 'random_flip']\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Original\n",
    "    ax = fig.add_subplot(141, projection='3d')\n",
    "    ax.scatter(data['points'][:, 0], data['points'][:, 1], data['points'][:, 2], \n",
    "              c=data['instance_labels'], cmap='tab20', s=1)\n",
    "    ax.set_title('Original')\n",
    "    \n",
    "    # Augmented versions\n",
    "    for i, aug_name in enumerate(augmentations, 2):\n",
    "        # Apply single augmentation\n",
    "        augmented_data = augmenter._augment_single(data.copy(), aug_name)\n",
    "        \n",
    "        ax = fig.add_subplot(1, 4, i, projection='3d')\n",
    "        ax.scatter(augmented_data['points'][:, 0],\n",
    "                  augmented_data['points'][:, 1],\n",
    "                  augmented_data['points'][:, 2],\n",
    "                  c=augmented_data['instance_labels'],\n",
    "                  cmap='tab20', s=1)\n",
    "        ax.set_title(aug_name)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize augmentations for a sample scene\n",
    "sample_data = dataset[0]\n",
    "visualize_augmentations(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Scene Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scene_complexity():\n",
    "    \"\"\"Analyze scene complexity metrics across the dataset.\"\"\"\n",
    "    complexity_metrics = {\n",
    "        'num_instances': [],\n",
    "        'instance_density': [],  # instances per cubic meter\n",
    "        'point_density': [],     # points per cubic meter\n",
    "        'volume': [],            # scene volume\n",
    "        'instance_overlap': []   # average number of instances in local neighborhoods\n",
    "    }\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc='Analyzing scene complexity'):\n",
    "        data = dataset[i]\n",
    "        points = data['points']\n",
    "        instance_labels = data['instance_labels']\n",
    "        \n",
    "        # Calculate scene volume\n",
    "        bounds = torch.max(points, dim=0)[0] - torch.min(points, dim=0)[0]\n",
    "        volume = bounds.prod().item()\n",
    "        complexity_metrics['volume'].append(volume)\n",
    "        \n",
    "        # Count instances\n",
    "        num_instances = len(torch.unique(instance_labels)) - 1  # exclude background\n",
    "        complexity_metrics['num_instances'].append(num_instances)\n",
    "        \n",
    "        # Calculate densities\n",
    "        complexity_metrics['instance_density'].append(num_instances / volume)\n",
    "        complexity_metrics['point_density'].append(len(points) / volume)\n",
    "        \n",
    "        # Calculate instance overlap\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points.numpy())\n",
    "        kdtree = o3d.geometry.KDTreeFlann(pcd)\n",
    "        \n",
    "        overlap_counts = []\n",
    "        sample_points = np.random.choice(len(points), size=min(100, len(points)))\n",
    "        \n",
    "        for idx in sample_points:\n",
    "            [_, neighbors, _] = kdtree.search_radius_vector_3d(pcd.points[idx], 0.5)\n",
    "            overlap_counts.append(len(torch.unique(instance_labels[neighbors])))\n",
    "            \n",
    "        complexity_metrics['instance_overlap'].append(np.mean(overlap_counts))\n",
    "    \n",
    "    # Plot complexity metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    \n",
    "    axes[0, 0].hist(complexity_metrics['num_instances'], bins=30)\n",
    "    axes[0, 0].set_title('Number of Instances')\n",
    "    \n",
    "    axes[0, 1].hist(complexity_metrics['instance_density'], bins=30)\n",
    "    axes[0, 1].set_title('Instance Density')\n",
    "    \n",
    "    axes[1, 0].hist(complexity_metrics['point_density'], bins=30)\n",
    "    axes[1, 0].set_title('Point Density')\n",
    "    \n",
    "    axes[1, 1].hist(complexity_metrics['instance_overlap'], bins=30)\n",
    "    axes[1, 1].set_title('Average Instance Overlap')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return complexity_metrics\n",
    "\n",
    "complexity_metrics = analyze_scene_complexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_summary(stats, complexity_metrics):\n",
    "    \"\"\"Print summary of dataset analysis.\"\"\"\n",
    "    print(\"Dataset Summary:\")\n",
    "    print(\"===============\")\n",
    "    print(f\"Number of scenes: {stats['num_scenes']}\")\n",
    "    print(f\"Number of semantic classes: {len(stats['semantic_classes'])}\")\n",
    "    print(\"\\nScene Statistics:\")\n",
    "    print(\"----------------\")\n",
    "    print(f\"Average points per scene: {np.mean(stats['points_per_scene']):.2f} ± {np.std(stats['points_per_scene']):.2f}\")\n",
    "    print(f\"Average instances per scene: {np.mean(stats['instances_per_scene']):.2f} ± {np.std(stats['instances_per_scene']):.2f}\")\n",
    "    \n",
    "    print(\"\\nComplexity Metrics:\")\n",
    "    print(\"-----------------\")\n",
    "    print(f\"Average instance density: {np.mean(complexity_metrics['instance_density']):.2f} instances/m³\")\n",
    "    print(f\"Average point density: {np.mean(complexity_metrics['point_density']):.2f} points/m³\")\n",
    "    print(f\"Average instance overlap: {np.mean(complexity_metrics['instance_overlap']):.2f} instances\")\n",
    "    \n",
    "    print(\"\\nKey Observations:\")\n",
    "    print(\"----------------\")\n",
    "    print(\"1. Instance Distribution:\")\n",
    "    print(f\"   - Most scenes have between {np.percentile(stats['instances_per_scene'], 25):.0f} and \"\n",
    "          f\"{np.percentile(stats['instances_per_scene'], 75):.0f} instances\")\n",
    "    \n",
    "    print(\"\\n2. Point Cloud Density:\")\n",
    "    print(f\"   - Point density varies significantly across scenes\")\n",
    "    print(f\"   - {np.percentile(complexity_metrics['point_density'], 90):.0f} points/m³ for the densest 10% of scenes\")\n",
    "    \n",
    "    print(\"\\n3. Instance Complexity:\")\n",
    "    print(f\"   - Average instance overlap suggests complex spatial relationships\")\n",
    "    print(f\"   - Instance density indicates challenging segmentation scenarios\")\n",
    "\n",
    "# Print summary\n",
    "print_dataset_summary(stats, complexity_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(dataset, num_samples=1000):\n",
    "    \"\"\"Analyze feature importance using simple statistical measures.\"\"\"\n",
    "    # Collect features and labels\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in tqdm(range(min(num_samples, len(dataset))), desc='Collecting features'):\n",
    "        data = dataset[i]\n",
    "        if 'geometric_features' in data and data['geometric_features'] is not None:\n",
    "            features = torch.cat([\n",
    "                data['geometric_features'],\n",
    "                data['contextual_features']\n",
    "            ], dim=1) if 'contextual_features' in data else data['geometric_features']\n",
    "            \n",
    "            all_features.append(features)\n",
    "            all_labels.append(data['instance_labels'])\n",
    "    \n",
    "    features = torch.cat(all_features, dim=0).numpy()\n",
    "    labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    \n",
    "    # Calculate feature statistics\n",
    "    feature_stats = {\n",
    "        'variance': np.var(features, axis=0),\n",
    "        'instance_separation': []\n",
    "    }\n",
    "    \n",
    "    # Calculate instance separation power\n",
    "    for i in range(features.shape[1]):\n",
    "        unique_instances = np.unique(labels)\n",
    "        instance_means = np.array([np.mean(features[labels == inst, i]) \n",
    "                                  for inst in unique_instances])\n",
    "        separation = np.var(instance_means) / np.var(features[:, i])\n",
    "        feature_stats['instance_separation'].append(separation)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Variance plot\n",
    "    ax1.bar(range(len(feature_stats['variance'])), feature_stats['variance'])\n",
    "    ax1.set_title('Feature Variance')\n",
    "    ax1.set_xlabel('Feature Index')\n",
    "    ax1.set_ylabel('Variance')\n",
    "    \n",
    "    # Instance separation plot\n",
    "    ax2.bar(range(len(feature_stats['instance_separation'])), \n",
    "            feature_stats['instance_separation'])\n",
    "    ax2.set_title('Instance Separation Power')\n",
    "    ax2.set_xlabel('Feature Index')\n",
    "    ax2.set_ylabel('Separation Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_stats\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_stats = analyze_feature_importance(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Instance Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_instance_relationships(data):\n",
    "    \"\"\"Analyze spatial relationships between instances.\"\"\"\n",
    "    points = data['points']\n",
    "    instance_labels = data['instance_labels']\n",
    "    unique_instances = torch.unique(instance_labels)\n",
    "    \n",
    "    # Calculate instance centroids and bounding boxes\n",
    "    centroids = {}\n",
    "    bboxes = {}\n",
    "    \n",
    "    for inst_id in unique_instances:\n",
    "        if inst_id == 0:  # Skip background\n",
    "            continue\n",
    "            \n",
    "        mask = (instance_labels == inst_id)\n",
    "        inst_points = points[mask]\n",
    "        \n",
    "        centroids[inst_id.item()] = inst_points.mean(0)\n",
    "        bboxes[inst_id.item()] = {\n",
    "            'min': inst_points.min(0)[0],\n",
    "            'max': inst_points.max(0)[0]\n",
    "        }\n",
    "    \n",
    "    # Calculate instance relationships\n",
    "    relationships = []\n",
    "    \n",
    "    for id1 in centroids.keys():\n",
    "        for id2 in centroids.keys():\n",
    "            if id1 >= id2:\n",
    "                continue\n",
    "                \n",
    "            # Calculate centroid distance\n",
    "            dist = torch.norm(centroids[id1] - centroids[id2])\n",
    "            \n",
    "            # Check for overlap\n",
    "            overlap = True\n",
    "            for dim in range(3):\n",
    "                if (bboxes[id1]['min'][dim] > bboxes[id2]['max'][dim] or\n",
    "                    bboxes[id1]['max'][dim] < bboxes[id2]['min'][dim]):\n",
    "                    overlap = False\n",
    "                    break\n",
    "                    \n",
    "            relationships.append({\n",
    "                'instance1': id1,\n",
    "                'instance2': id2,\n",
    "                'distance': dist.item(),\n",
    "                'overlapping': overlap\n",
    "            })\n",
    "    \n",
    "    # Visualize relationships\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Distance distribution\n",
    "    distances = [r['distance'] for r in relationships]\n",
    "    plt.subplot(121)\n",
    "    plt.hist(distances, bins=30)\n",
    "    plt.title('Instance Distance Distribution')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Overlap statistics\n",
    "    overlap_count = sum(r['overlapping'] for r in relationships)\n",
    "    plt.subplot(122)\n",
    "    plt.pie([overlap_count, len(relationships) - overlap_count],\n",
    "            labels=['Overlapping', 'Non-overlapping'],\n",
    "            autopct='%1.1f%%')\n",
    "    plt.title('Instance Overlap Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "# Analyze instance relationships for a sample scene\n",
    "sample_data = dataset[0]\n",
    "relationships = analyze_instance_relationships(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Recommendations for Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis above, here are key considerations for model design:\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Geometric features show strong instance separation power\n",
    "   - Consider using multi-scale features due to varying instance sizes\n",
    "   - Include local density information due to varying point density\n",
    "\n",
    "2. **Architecture Choices**:\n",
    "   - Need to handle varying number of instances per scene\n",
    "   - Consider attention mechanisms for complex spatial relationships\n",
    "   - Include multi-resolution processing for different instance sizes\n",
    "\n",
    "3. **Training Strategy**:\n",
    "   - Use instance overlap awareness in loss function\n",
    "   - Consider curriculum learning based on scene complexity\n",
    "   - Implement robust data augmentation due to spatial variations\n",
    "\n",
    "4. **Evaluation Metrics**:\n",
    "   - Include metrics for different instance sizes\n",
    "   - Consider boundary quality metrics\n",
    "   - Evaluate performance on overlapping instances separately"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
