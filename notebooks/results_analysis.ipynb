{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud Instance Segmentation Results Analysis\n",
    "\n",
    "This notebook analyzes the performance of different models and configurations for point cloud instance segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.evaluation import evaluate_predictions\n",
    "from src.evaluation.visualizer import PointCloudVisualizer\n",
    "from src.models import PointNet2InstanceSegmentation, SparseCNNInstanceSegmentation\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_results(results_dir: str):\n",
    "    \"\"\"Load results from multiple experiments.\"\"\"\n",
    "    results_dir = Path(results_dir)\n",
    "    \n",
    "    results = {\n",
    "        'pointnet2': {},\n",
    "        'sparsecnn': {}\n",
    "    }\n",
    "    \n",
    "    # Load results for each model type\n",
    "    for model_type in results.keys():\n",
    "        model_dir = results_dir / model_type\n",
    "        if not model_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        # Load each experiment\n",
    "        for exp_dir in model_dir.glob('*'):\n",
    "            if not exp_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            # Load config\n",
    "            config_path = exp_dir / 'config.yaml'\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "                \n",
    "            # Load metrics\n",
    "            metrics_path = exp_dir / 'evaluation_results.yaml'\n",
    "            with open(metrics_path, 'r') as f:\n",
    "                metrics = yaml.safe_load(f)\n",
    "                \n",
    "            results[model_type][exp_dir.name] = {\n",
    "                'config': config,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load results\n",
    "results = load_experiment_results('../results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_performance(results):\n",
    "    \"\"\"Compare performance metrics across different models and configurations.\"\"\"\n",
    "    # Prepare data for visualization\n",
    "    data = []\n",
    "    \n",
    "    for model_type, experiments in results.items():\n",
    "        for exp_name, exp_results in experiments.items():\n",
    "            metrics = exp_results['metrics']\n",
    "            \n",
    "            data.append({\n",
    "                'Model': model_type,\n",
    "                'Experiment': exp_name,\n",
    "                'mAP': metrics['mAP'],\n",
    "                'IoU': metrics['IoU'],\n",
    "                'Semantic Accuracy': metrics.get('semantic_accuracy', 0)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create comparative visualizations\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # mAP comparison\n",
    "    sns.barplot(data=df, x='Model', y='mAP', ax=axes[0])\n",
    "    axes[0].set_title('Mean Average Precision')\n",
    "    \n",
    "    # IoU comparison\n",
    "    sns.barplot(data=df, x='Model', y='IoU', ax=axes[1])\n",
    "    axes[1].set_title('Mean IoU')\n",
    "    \n",
    "    # Semantic accuracy comparison\n",
    "    sns.barplot(data=df, x='Model', y='Semantic Accuracy', ax=axes[2])\n",
    "    axes[2].set_title('Semantic Segmentation Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\nDetailed Statistics:\")\n",
    "    print(\"===================\\n\")\n",
    "    \n",
    "    for metric in ['mAP', 'IoU', 'Semantic Accuracy']:\n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(df.groupby('Model')[metric].describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "performance_df = compare_model_performance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instance Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_instance_size_performance(results):\n",
    "    \"\"\"Analyze model performance across different instance sizes.\"\"\"\n",
    "    size_categories = ['small', 'medium', 'large']\n",
    "    \n",
    "    # Collect performance data by instance size\n",
    "    size_data = []\n",
    "    \n",
    "    for model_type, experiments in results.items():\n",
    "        for exp_name, exp_results in experiments.items():\n",
    "            metrics = exp_results['metrics']\n",
    "            \n",
    "            if 'size_metrics' in metrics:\n",
    "                for size in size_categories:\n",
    "                    size_data.append({\n",
    "                        'Model': model_type,\n",
    "                        'Experiment': exp_name,\n",
    "                        'Size': size,\n",
    "                        'mAP': metrics['size_metrics'][f'{size}_mAP'],\n",
    "                        'IoU': metrics['size_metrics'][f'{size}_IoU']\n",
    "                    })\n",
    "    \n",
    "    df = pd.DataFrame(size_data)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # mAP by instance size\n",
    "    sns.boxplot(data=df, x='Size', y='mAP', hue='Model', ax=ax1)\n",
    "    ax1.set_title('mAP by Instance Size')\n",
    "    \n",
    "    # IoU by instance size\n",
    "    sns.boxplot(data=df, x='Size', y='IoU', hue='Model', ax=ax2)\n",
    "    ax2.set_title('IoU by Instance Size')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(\"\\nPerformance Statistics by Instance Size:\")\n",
    "    print(\"=====================================\\n\")\n",
    "    \n",
    "    for metric in ['mAP', 'IoU']:\n",
    "        print(f\"\\n{metric} by Size:\")\n",
    "        print(df.groupby(['Model', 'Size'])[metric].describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "size_performance_df = analyze_instance_size_performance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_training_convergence():\n",
    "    \"\"\"Analyze training convergence using W&B logs.\"\"\"\n",
    "    # Connect to W&B\n",
    "    api = wandb.Api()\n",
    "    \n",
    "    # Get runs for each model\n",
    "    models = ['pointnet2', 'sparsecnn']\n",
    "    runs_data = {}\n",
    "    \n",
    "    for model in models:\n",
    "        runs = api.runs(f\"your-project/{model}\")\n",
    "        runs_data[model] = []\n",
    "        \n",
    "        for run in runs:\n",
    "            history = pd.DataFrame(run.scan_history())\n",
    "            runs_data[model].append({\n",
    "                'run_name': run.name,\n",
    "                'history': history\n",
    "            })\n",
    "    \n",
    "    # Plot training curves\n",
    "    metrics = ['loss', 'val_loss', 'mAP', 'val_mAP']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        \n",
    "        for model in models:\n",
    "            for run_data in runs_data[model]:\n",
    "                history = run_data['history']\n",
    "                if metric in history.columns:\n",
    "                    ax.plot(history['epoch'], history[metric],\n",
    "                           label=f\"{model}-{run_data['run_name']}\",\n",
    "                           alpha=0.7)\n",
    "                    \n",
    "        ax.set_title(f'{metric} vs Epoch')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze convergence statistics\n",
    "    convergence_stats = {}\n",
    "    \n",
    "    for model in models:\n",
    "        convergence_stats[model] = {\n",
    "            'epochs_to_converge': [],\n",
    "            'final_performance': []\n",
    "        }\n",
    "        \n",
    "        for run_data in runs_data[model]:\n",
    "            history = run_data['history']\n",
    "            \n",
    "            # Calculate epochs to convergence\n",
    "            if 'val_loss' in history.columns:\n",
    "                smoothed_loss = history['val_loss'].rolling(window=5).mean()\n",
    "                converged_epoch = len(smoothed_loss)\n",
    "                \n",
    "                for i in range(len(smoothed_loss) - 5):\n",
    "                    if abs(smoothed_loss.iloc[i:i+5].mean() - \n",
    "                          smoothed_loss.iloc[i+5:i+10].mean()) < 0.001:\n",
    "                        converged_epoch = i\n",
    "                        break\n",
    "                        \n",
    "                convergence_stats[model]['epochs_to_converge'].append(converged_epoch)\n",
    "                \n",
    "            # Record final performance\n",
    "            if 'val_mAP' in history.columns:\n",
    "                final_map = history['val_mAP'].iloc[-5:].mean()\n",
    "                convergence_stats[model]['final_performance'].append(final_map)\n",
    "    \n",
    "    print(\"\\nConvergence Statistics:\")\n",
    "    print(\"=====================\\n\")\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\n{model.upper()}:\")\n",
    "        print(f\"Average epochs to converge: \"\n",
    "              f\"{np.mean(convergence_stats[model]['epochs_to_converge']):.1f} ± \"\n",
    "              f\"{np.std(convergence_stats[model]['epochs_to_converge']):.1f}\")\n",
    "        print(f\"Final mAP: \"\n",
    "              f\"{np.mean(convergence_stats[model]['final_performance']):.3f} ± \"\n",
    "              f\"{np.std(convergence_stats[model]['final_performance']):.3f}\")\n",
    "    \n",
    "    return runs_data, convergence_stats\n",
    "\n",
    "runs_data, convergence_stats = analyze_training_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(results):\n",
    "    \"\"\"Analyze error patterns and failure cases.\"\"\"\n",
    "    error_patterns = {\n",
    "        'over_segmentation': [],\n",
    "        'under_segmentation': [],\n",
    "        'boundary_errors': [],\n",
    "        'classification_errors': []\n",
    "    }\n",
    "    \n",
    "    # Load validation predictions\n",
    "    for model_type, experiments in results.items():\n",
    "        for exp_name, exp_results in experiments.items():\n",
    "            if 'error_analysis' in exp_results['metrics']:\n",
    "                error_analysis = exp_results['metrics']['error_analysis']\n",
    "                \n",
    "                error_patterns['over_segmentation'].append({\n",
    "                    'model': model_type,\n",
    "                    'experiment': exp_name,\n",
    "                    'rate': error_analysis['over_segmentation_rate']\n",
    "                })\n",
    "                \n",
    "                error_patterns['under_segmentation'].append({\n",
    "                    'model': model_type,\n",
    "                    'experiment': exp_name,\n",
    "                    'rate': error_analysis['under_segmentation_rate']\n",
    "                })\n",
    "                \n",
    "                error_patterns['boundary_errors'].append({\n",
    "                    'model': model_type,\n",
    "                    'experiment': exp_name,\n",
    "                    'rate': error_analysis['boundary_error_rate']\n",
    "                })\n",
    "                \n",
    "                error_patterns['classification_errors'].append({\n",
    "                    'model': model_type,\n",
    "                    'experiment': exp_name,\n",
    "                    'rate': error_analysis['classification_error_rate']\n",
    "                })\n",
    "    \n",
    "    # Visualize error patterns\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    for i, (error_type, data) in enumerate(error_patterns.items()):\n",
    "        if not data:  # Skip if no data available\n",
    "            continue\n",
    "            \n",
    "        df = pd.DataFrame(data)\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        \n",
    "        sns.boxplot(data=df, x='model', y='rate', ax=ax)\n",
    "        ax.set_title(f'{error_type.replace(\"_\", \" \").title()} Rate')\n",
    "        ax.set_ylabel('Error Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"Error Pattern Analysis:\")\n",
    "    print(\"=====================\\n\")\n",
    "    \n",
    "    for error_type, data in error_patterns.items():\n",
    "        if not data:\n",
    "            continue\n",
    "            \n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"\\n{error_type.replace('_', ' ').title()}:\")\n",
    "        print(df.groupby('model')['rate'].describe())\n",
    "    \n",
    "    return error_patterns\n",
    "\n",
    "error_patterns = analyze_errors(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Instance Boundary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_boundary_quality(results):\n",
    "    \"\"\"Analyze instance boundary quality and precision.\"\"\"\n",
    "    boundary_metrics = []\n",
    "    \n",
    "    for model_type, experiments in results.items():\n",
    "        for exp_name, exp_results in experiments.items():\n",
    "            if 'boundary_metrics' in exp_results['metrics']:\n",
    "                metrics = exp_results['metrics']['boundary_metrics']\n",
    "                \n",
    "                boundary_metrics.append({\n",
    "                    'model': model_type,\n",
    "                    'experiment': exp_name,\n",
    "                    'precision': metrics['boundary_precision'],\n",
    "                    'recall': metrics['boundary_recall'],\n",
    "                    'f1': metrics['boundary_f1'],\n",
    "                    'accuracy': metrics['boundary_accuracy']\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(boundary_metrics)\n",
    "    \n",
    "    # Visualize boundary metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    metrics = ['precision', 'recall', 'f1', 'accuracy']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        sns.boxplot(data=df, x='model', y=metric, ax=ax)\n",
    "        ax.set_title(f'Boundary {metric.title()}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create precision-recall curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    for model in df['model'].unique():\n",
    "        model_data = df[df['model'] == model]\n",
    "        plt.scatter(model_data['recall'], model_data['precision'],\n",
    "                   label=model, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Boundary Precision-Recall')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "boundary_df = analyze_boundary_quality(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Computational Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_computational_performance(results):\n",
    "    \"\"\"Analyze computational efficiency and resource usage.\"\"\"\n",
    "    performance_metrics = []\n",
    "    \n",
    "    for model_type, experiments in results.items():\n",
    "        for exp_name, exp_results in experiments.items():\n",
    "            if 'performance_metrics' in exp_results['metrics']:\n",
    "                metrics = exp_results['metrics']['performance_metrics']\n",
    "                \n",
    "                performance_metrics.append({\n",
    "                    'model': model_type,\n",
    "                    'experiment': exp_name,\n",
    "                    'inference_time': metrics['inference_time'],\n",
    "                    'memory_usage': metrics['memory_usage'],\n",
    "                    'flops': metrics['flops'],\n",
    "                    'parameters': metrics['parameters']\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(performance_metrics)\n",
    "    \n",
    "    # Visualize performance metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    metrics = ['inference_time', 'memory_usage', 'flops', 'parameters']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        sns.boxplot(data=df, x='model', y=metric, ax=ax)\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create performance trade-off plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for model in df['model'].unique():\n",
    "        model_data = df[df['model'] == model]\n",
    "        plt.scatter(model_data['inference_time'], \n",
    "                   model_data['memory_usage'],\n",
    "                   s=model_data['parameters'] / 1e4,\n",
    "                   label=model, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Inference Time (ms)')\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "    plt.title('Performance Trade-off Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"Performance Statistics:\")\n",
    "    print(\"=====================\\n\")\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "        print(df.groupby('model')[metric].describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "performance_df = analyze_computational_performance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ablation Study Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ablation_studies(results):\n",
    "    \"\"\"Analyze the impact of different model components and configurations.\"\"\"\n",
    "    ablation_results = []\n",
    "    \n",
    "    for model_type, experiments in results.items():\n",
    "        for exp_name, exp_results in experiments.items():\n",
    "            config = exp_results['config']\n",
    "            metrics = exp_results['metrics']\n",
    "            \n",
    "            # Extract key configuration parameters\n",
    "            ablation_results.append({\n",
    "                'model': model_type,\n",
    "                'experiment': exp_name,\n",
    "                'feature_type': config['model'].get('feature_type', 'default'),\n",
    "                'attention': config['model'].get('use_attention', False),\n",
    "                'multi_scale': config['model'].get('multi_scale', False),\n",
    "                'mAP': metrics['mAP'],\n",
    "                'IoU': metrics['IoU']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(ablation_results)\n",
    "    \n",
    "    # Analyze impact of different components\n",
    "    component_analysis = []\n",
    "    \n",
    "    # Feature type analysis\n",
    "    feature_impact = df.groupby(['model', 'feature_type'])['mAP'].mean().unstack()\n",
    "    component_analysis.append(('Feature Type', feature_impact))\n",
    "    \n",
    "    # Attention mechanism analysis\n",
    "    attention_impact = df.groupby(['model', 'attention'])['mAP'].mean().unstack()\n",
    "    component_analysis.append(('Attention', attention_impact))\n",
    "    \n",
    "    # Multi-scale analysis\n",
    "    scale_impact = df.groupby(['model', 'multi_scale'])['mAP'].mean().unstack()\n",
    "    component_analysis.append(('Multi-Scale', scale_impact))\n",
    "    \n",
    "    # Visualize component impact\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, (component, impact) in enumerate(component_analysis):\n",
    "        impact.plot(kind='bar', ax=axes[i])\n",
    "        axes[i].set_title(f'Impact of {component}')\n",
    "        axes[i].set_ylabel('mAP')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical significance testing\n",
    "    print(\"Statistical Significance Analysis:\")\n",
    "    print(\"===============================\\n\")\n",
    "    \n",
    "    from scipy import stats\n",
    "    \n",
    "    for component in ['feature_type', 'attention', 'multi_scale']:\n",
    "        print(f\"\\n{component.replace('_', ' ').title()} Impact:\")\n",
    "        \n",
    "        for model in df['model'].unique():\n",
    "            values = df[df['model'] == model][component].unique()\n",
    "            if len(values) > 1:\n",
    "                groups = [df[(df['model'] == model) & (df[component] == val)]['mAP']\n",
    "                         for val in values]\n",
    "                f_stat, p_value = stats.f_oneway(*groups)\n",
    "                print(f\"{model}: F-statistic = {f_stat:.3f}, p-value = {p_value:.3f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "ablation_df = analyze_ablation_studies(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_best_worst_cases(results):\n",
    "    \"\"\"Visualize best and worst performing examples.\"\"\"\n",
    "    visualizer = PointCloudVisualizer(config=None)\n",
    "    \n",
    "    for model_type, experiments in results.items():\n",
    "        for exp_name, exp_results in experiments.items():\n",
    "            if 'case_studies' in exp_results['metrics']:\n",
    "                cases = exp_results['metrics']['case_studies']\n",
    "                \n",
    "                # Visualize best case\n",
    "                print(f\"\\nBest Case for {model_type} - {exp_name}:\")\n",
    "                visualizer.create_comparison_visualization(\n",
    "                    points=cases['best_case']['points'],\n",
    "                    pred_instances=cases['best_case']['predictions'],\n",
    "                    gt_instances=cases['best_case']['ground_truth'],\n",
    "                    filename=f'best_case_{model_type}_{exp_name}'\n",
    "                )\n",
    "                \n",
    "                # Visualize worst case\n",
    "                print(f\"\\nWorst Case for {model_type} - {exp_name}:\")\n",
    "                visualizer.create_comparison_visualization(\n",
    "                    points=cases['worst_case']['points'],\n",
    "                    pred_instances=cases['worst_case']['predictions'],\n",
    "                    gt_instances=cases['worst_case']['ground_truth'],\n",
    "                    filename=f'worst_case_{model_type}_{exp_name}'\n",
    "                )\n",
    "\n",
    "visualize_best_worst_cases(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Ensemble Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ensemble_performance(results):\n",
    "    \"\"\"Analyze performance of model ensembles.\"\"\"\n",
    "    if 'ensemble' not in results:\n",
    "        print(\"No ensemble results available.\")\n",
    "        return\n",
    "        \n",
    "    ensemble_metrics = []\n",
    "    individual_metrics = []\n",
    "    \n",
    "    # Collect ensemble and individual model metrics\n",
    "    for exp_name, exp_results in results['ensemble'].items():\n",
    "        metrics = exp_results['metrics']\n",
    "        \n",
    "        ensemble_metrics.append({\n",
    "            'experiment': exp_name,\n",
    "            'mAP': metrics['mAP'],\n",
    "            'IoU': metrics['IoU']\n",
    "        })\n",
    "        \n",
    "        # Collect individual model performances\n",
    "        for model in ['pointnet2', 'sparsecnn']:\n",
    "            if model in results and exp_name in results[model]:\n",
    "                individual_metrics.append({\n",
    "                    'model': model,\n",
    "                    'experiment': exp_name,\n",
    "                    'mAP': results[model][exp_name]['metrics']['mAP'],\n",
    "                    'IoU': results[model][exp_name]['metrics']['IoU']\n",
    "                })\n",
    "    \n",
    "    # Compare ensemble vs individual performance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot individual model performance\n",
    "    for model in ['pointnet2', 'sparsecnn']:\n",
    "        model_data = pd.DataFrame([m for m in individual_metrics if m['model'] == model])\n",
    "        plt.scatter(model_data['mAP'], model_data['IoU'],\n",
    "                   label=f'Individual {model}', alpha=0.6)\n",
    "    \n",
    "    # Plot ensemble performance\n",
    "    ensemble_data = pd.DataFrame(ensemble_metrics)\n",
    "    plt.scatter(ensemble_data['mAP'], ensemble_data['IoU'],\n",
    "               label='Ensemble', marker='*', s=200)\n",
    "    \n",
    "    plt.xlabel('mAP')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.title('Ensemble vs Individual Model Performance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print performance comparison\n",
    "    print(\"Performance Comparison:\")\n",
    "    print(\"=====================\\n\")\n",
    "    \n",
    "    print(\"Ensemble Performance:\")\n",
    "    print(pd.DataFrame(ensemble_metrics).describe())\n",
    "    \n",
    "    print(\"\\nIndividual Model Performance:\")\n",
    "    print(pd.DataFrame(individual_metrics).groupby('model').describe())\n",
    "\n",
    "analyze_ensemble_performance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the comprehensive analysis above, here are the key findings and recommendations:\n",
    "\n",
    "### Model Selection\n",
    "1. **Best Overall Performance**:\n",
    "   - Model: [Best performing model]\n",
    "   - Configuration: [Optimal configuration]\n",
    "   - Key metrics: mAP = [value], IoU = [value]\n",
    "\n",
    "2. **Trade-offs**:\n",
    "   - Performance vs. Speed: [Analysis]\n",
    "   - Memory vs. Accuracy: [Analysis]\n",
    "   - Complexity vs. Results: [Analysis]\n",
    "\n",
    "### Key Component Findings\n",
    "1. **Feature Engineering**:\n",
    "   - Most effective features: [List]\n",
    "   - Recommended combinations: [List]\n",
    "\n",
    "2. **Architecture Choices**:\n",
    "   - Critical components: [List]\n",
    "   - Optional enhancements: [List]\n",
    "\n",
    "### Practical Implementation Recommendations\n",
    "1. **Training Strategy**:\n",
    "   - Batch size: [value]\n",
    "   - Learning rate schedule: [details]\n",
    "   - Data augmentation: [recommendations]\n",
    "\n",
    "2. **Deployment Considerations**:\n",
    "   - Memory requirements: [details]\n",
    "   - Inference optimization: [strategies]\n",
    "   - Hardware requirements: [specifications]\n",
    "\n",
    "### Future Improvements\n",
    "1. **Research Directions**:\n",
    "   - [Potential improvement 1]\n",
    "   - [Potential improvement 2]\n",
    "   - [Potential improvement 3]\n",
    "\n",
    "2. **Technical Enhancements**:\n",
    "   - [Enhancement 1]\n",
    "   - [Enhancement 2]\n",
    "   - [Enhancement 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_results(results_dict):\n",
    "    \"\"\"Export analysis results to various formats.\"\"\"\n",
    "    output_dir = Path('../results/analysis')\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Save performance metrics to CSV\n",
    "    performance_df.to_csv(output_dir / 'performance_metrics.csv')\n",
    "    \n",
    "    # Save error analysis to CSV\n",
    "    pd.DataFrame(error_patterns).to_csv(output_dir / 'error_analysis.csv')\n",
    "    \n",
    "    # Save ablation study results\n",
    "    ablation_df.to_csv(output_dir / 'ablation_study.csv')\n",
    "    \n",
    "    # Create summary report\n",
    "    with open(output_dir / 'analysis_summary.md', 'w') as f:\n",
    "        f.write('# Instance Segmentation Analysis Summary\\n\\n')\n",
    "        \n",
    "        f.write('## Performance Metrics\\n')\n",
    "        f.write(performance_df.describe().to_markdown())\n",
    "        \n",
    "        f.write('\\n## Error Analysis\\n')\n",
    "        f.write(pd.DataFrame(error_patterns).describe().to_markdown())\n",
    "        \n",
    "        f.write('\\n## Ablation Study Results\\n')\n",
    "        f.write(ablation_df.describe().to_markdown())\n",
    "    \n",
    "    print(f\"Analysis results exported to {output_dir}\")\n",
    "\n",
    "# Export all results\n",
    "export_analysis_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
